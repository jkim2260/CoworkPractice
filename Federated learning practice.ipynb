{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2649385182.py, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 65\u001b[1;36m\u001b[0m\n\u001b[1;33m    for i in range(len(y_data))\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression #Logistic(Regression)Classifier\n",
    "\n",
    "class FedLearning(object):\n",
    "    \n",
    "    #Loading iris data and shuffling for spliting data to clients\n",
    "    def __init__(self, obj_data):\n",
    "        self.data = obj_data        \n",
    "        df = pd.DataFrame(data = self.data.data, columns = self.data.feature_names)\n",
    "        df['target'] = self.data.target\n",
    "        df['target'] = df['target'].map({0:\"setosa\", 1:\"versicolor\", 2:\"virginica\"})\n",
    "        df = shuffle(df)\n",
    "        self.x_data = df.iloc[:, :-1]\n",
    "        self.y_data = df.iloc[:, [-1]]\n",
    "    \n",
    "    #Spliting data by the number of clients\n",
    "    def client_data(self, num):\n",
    "        client = [];client_ans = []\n",
    "        split_loc = len(self.x_data)//num\n",
    "        for i in range(num):\n",
    "            temp_x = self.x_data.iloc[split_loc*i:split_loc*(i+1), :]\n",
    "            temp_y = self.y_data.iloc[split_loc*i:split_loc*(i+1), :]\n",
    "            client.append(temp_x)\n",
    "            client_ans.append(temp_y)\n",
    "        #print(client_ans, len(client_ans[0]))\n",
    "        return client, client_ans\n",
    "    \n",
    "    #initialize weight of model\n",
    "    def weight_ini(self):\n",
    "        #.shape[1] is the number of columns\n",
    "        #.shape[0] is the number of rows\n",
    "        n_features = self.x_data.shape[1]\n",
    "        w = np.zeros((1,n_features))\n",
    "        b = 0\n",
    "        print(w, b)                   \n",
    "        return w, b\n",
    "    \n",
    "    #Average weights and bias learned using client data\n",
    "    #client_weight and client_bias are array structure\n",
    "    def server_aggre(self, client_weight, client_bias):\n",
    "        Avg_weight = np.mean(client_weight, axis = 0)\n",
    "        Avg_bias = np.mean(client_bias)\n",
    "        return Avg_weight, Avg_bias\n",
    "    \n",
    "    def client_train(self, weight, bias, client, client_ans, iteration):\n",
    "        #client data train 60% validation 20% test 20%\n",
    "        \n",
    "        return weight, bias\n",
    "    \n",
    "    def model_optimize(w, b, X, Y):\n",
    "        return\n",
    "    \n",
    "        \n",
    "    #I use the multinomial LR model instead of binomial LR\n",
    "    #becasuse the dataset has three classes(multi-class)\n",
    "    #change loss func(log loss to cross-entropy loss)\n",
    "    #and change the sigmoid func to softmax func\n",
    "    def softmax(self, result):\n",
    "        final_result = np.exp(result).T / np.sum(np.exp(result), axis = 1).T\n",
    "        return final_result\n",
    "    \n",
    "    #One hot encoding converts the categorical data into a numerical form\n",
    "    def one_hot_encoding(self, y_data):\n",
    "        OneHotEncoding = []\n",
    "        encoding = []\n",
    "        for i in range(len(y_data)):\n",
    "            if(y_data[i] == 0):\n",
    "                encoding = np.array([1,0,0]) #class 1(setosa)\n",
    "            elif(y_data[i] == 1):\n",
    "                encoding = np.array([0,1,0]) #class 2(versicolor)\n",
    "            elif(y_data[i] == 2):\n",
    "                encoding = np.array([0,0,1]) #class 3(virginica)\n",
    "            OneHotEncoding.append(encoding)\n",
    "        return\n",
    "    \n",
    "    #use cross entropy for calculating loss value\n",
    "    def cross_entropy(self, y_data, y_pred):\n",
    "        loss = -np.sum(y_data * np.log(y_pred + 10**-100))\n",
    "        return loss\n",
    "    \n",
    "    #jacobian of cross entropy\n",
    "    def cross_entropy_grad(self, y_data, y_pred):\n",
    "        grad = -y_data / (y_pred + 10**-100)\n",
    "        return grad\n",
    "\n",
    "    \n",
    "\n",
    "iris = load_iris()\n",
    "FL = FedLearning(iris)\n",
    "a = FL.client_data(3)\n",
    "(w,b) = FL.weight_ini()\n",
    "#print(a)\n",
    "\n",
    "\n",
    "'''\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_data, y_data.values.ravel())\n",
    "y_pred = LR.predict(x_data)\n",
    "print(\"LR's Accuracy is\", accuracy_score(y_data, y_pred))\n",
    "coef = LR.coef_\n",
    "intercept = LR.intercept_\n",
    "print(coef, intercept)\n",
    "   \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d9c63f866f60b974796075f5c09289b8fc4f57e9d727c901eeb59a008f9ea7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
